{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################ IMPORTING THE REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## EMOTIONS INCLUDED IN THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## EXTRACTING FEATURES FROM THE AUDIO SIGNAL USING LIBROSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma,spectral_centroid,spectral_bandwidth,spectral_rolloff,\n",
    "                    spectral_contrast,rms,spectral_flatness):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "            \n",
    "            \n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))             \n",
    "           \n",
    "            \n",
    "        if spectral_centroid:\n",
    "            spectral_centroid=np.mean(librosa.feature.spectral_centroid(y=X, sr=sample_rate))\n",
    "            result=np.hstack((result, spectral_centroid)) \n",
    "        \n",
    "        if spectral_bandwidth:\n",
    "           spectral_bandwidth=np.mean(librosa.feature.spectral_bandwidth(y=X, sr=sample_rate).T)\n",
    "#           print(spectral_bandwidth)\n",
    "           result=np.hstack((result, spectral_bandwidth)) \n",
    "           \n",
    "        if spectral_rolloff:\n",
    "           spectral_rolloff=np.mean(librosa.feature.spectral_rolloff(y=X, sr=sample_rate).T)\n",
    "#           print(spectral_rolloff)\n",
    "           result=np.hstack((result, spectral_rolloff))\n",
    "        \n",
    "        if spectral_contrast:\n",
    "           spectral_contrast=np.mean(librosa.feature.spectral_contrast(y=X, sr=sample_rate))\n",
    "           result=np.hstack((result, spectral_contrast))\n",
    "           \n",
    "        if rms:\n",
    "           rms=np.mean(librosa.feature.rms(y=X).T,axis=0)\n",
    "           result=np.hstack((result, rms))\n",
    "           \n",
    "        if spectral_flatness:\n",
    "           spectral_flatness=np.mean(librosa.feature.spectral_flatness(y=X))\n",
    "           result=np.hstack((result, spectral_flatness))\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## LOADING THE DATASET AND EXTRACTING ALL THE FEATURES FROM IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(r\"F:\\speech_project\\speech-emotion-recognition-ravdess-data\\Actor_*\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True,spectral_centroid=True,spectral_bandwidth=True,\n",
    "                                spectral_rolloff=True,spectral_contrast=True,rms=True,spectral_flatness=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    # Create scaler: scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X=scaler.fit_transform(x)\n",
    "    X=pd.DataFrame(X)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## SHUFFLING OF OBS AND RESETTING THE OBS INDEXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=load_data()\n",
    "X=pd.DataFrame(X)\n",
    "y=pd.DataFrame(y)\n",
    "data=pd.concat([X,y],axis=\"columns\")\n",
    "data=data.sample(frac=1).reset_index(drop=True)\n",
    "X=data.iloc[:,0:58]\n",
    "y=data.iloc[:,58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## LABEL ENCODING THE RESPONSE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## SPLITTING THE DATA INTO TRAIN(95%) AND TEST(5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1345, 71)\n",
      "Features extracted: 58\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X, y, test_size=0.05, random_state=9,stratify=y)\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "\n",
    "print(f'Features extracted: {x_train.shape[1]}')\n",
    "x_train=x_train.to_numpy()\n",
    "x_test=x_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############ Creating a data structure with 60 timesteps and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(0, 1345):\n",
    "    X_train.append(x_train[i].reshape(58,1))\n",
    "    Y_train.append(y_train[i])\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for j in range(0,71):\n",
    "    X_test.append(x_test[j].reshape(58,1))\n",
    "    Y_test.append(y_test[j])\n",
    "X_test, Y_test = np.array(X_test), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "X_train = np.reshape(np.array(X_train), (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "X_test = np.reshape(np.array(X_test), (X_test.shape[0], X_test.shape[1], 1))\n",
    "lb = LabelEncoder()\n",
    "Y_train = np_utils.to_categorical(lb.fit_transform(Y_train))\n",
    "Y_test = np_utils.to_categorical(lb.fit_transform(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################### LSTM RNN  ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM RNN model ...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 58, 452)           820832    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 58, 250)           703000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 58, 250)           501000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 120)               178080    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               18150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 1208      \n",
      "=================================================================\n",
      "Total params: 2,222,270\n",
      "Trainable params: 2,222,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build LSTM RNN model ...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=452, dropout=0.05, recurrent_dropout=0.20, return_sequences=True,input_shape = (X_train.shape[1],1)))\n",
    "model.add(LSTM(units=250, dropout=0.05, recurrent_dropout=0.20, return_sequences=True))\n",
    "model.add(LSTM(units=250, dropout=0.05, recurrent_dropout=0.20, return_sequences=True))\n",
    "model.add(LSTM(units=120, dropout=0.05, recurrent_dropout=0.20, return_sequences=False))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Training LSTM ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started..... please wait.\n",
      "Train on 1345 samples, validate on 71 samples\n",
      "Epoch 1/60\n",
      " - 29s - loss: 2.0765 - acc: 0.1115 - val_loss: 2.0639 - val_acc: 0.1549\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15493, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 2/60\n",
      " - 23s - loss: 2.0708 - acc: 0.1286 - val_loss: 2.0654 - val_acc: 0.1408\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.15493\n",
      "Epoch 3/60\n",
      " - 23s - loss: 2.0639 - acc: 0.1227 - val_loss: 2.0640 - val_acc: 0.1268\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.15493\n",
      "Epoch 4/60\n",
      " - 22s - loss: 2.0628 - acc: 0.1309 - val_loss: 2.0652 - val_acc: 0.1408\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.15493\n",
      "Epoch 5/60\n",
      " - 22s - loss: 2.0628 - acc: 0.1428 - val_loss: 2.0615 - val_acc: 0.1690\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.15493 to 0.16901, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 6/60\n",
      " - 24s - loss: 2.0621 - acc: 0.1331 - val_loss: 2.0601 - val_acc: 0.1690\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.16901\n",
      "Epoch 7/60\n",
      " - 24s - loss: 2.0604 - acc: 0.1301 - val_loss: 2.0587 - val_acc: 0.1408\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.16901\n",
      "Epoch 8/60\n",
      " - 24s - loss: 2.0583 - acc: 0.1390 - val_loss: 2.0634 - val_acc: 0.1549\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.16901\n",
      "Epoch 9/60\n",
      " - 24s - loss: 2.0611 - acc: 0.1390 - val_loss: 2.0531 - val_acc: 0.1972\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.16901 to 0.19718, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 10/60\n",
      " - 23s - loss: 2.0126 - acc: 0.1762 - val_loss: 2.0199 - val_acc: 0.1831\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.19718\n",
      "Epoch 11/60\n",
      " - 23s - loss: 1.9912 - acc: 0.2156 - val_loss: 1.9752 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.19718 to 0.25352, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 12/60\n",
      " - 23s - loss: 1.9370 - acc: 0.2290 - val_loss: 1.9387 - val_acc: 0.2254\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.25352\n",
      "Epoch 13/60\n",
      " - 24s - loss: 1.9054 - acc: 0.2454 - val_loss: 1.9726 - val_acc: 0.2254\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.25352\n",
      "Epoch 14/60\n",
      " - 23s - loss: 1.9041 - acc: 0.2476 - val_loss: 1.9305 - val_acc: 0.2254\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.25352\n",
      "Epoch 15/60\n",
      " - 23s - loss: 1.8774 - acc: 0.2602 - val_loss: 1.8727 - val_acc: 0.2113\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.25352\n",
      "Epoch 16/60\n",
      " - 23s - loss: 1.8843 - acc: 0.2483 - val_loss: 1.8489 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.25352\n",
      "Epoch 17/60\n",
      " - 25s - loss: 1.8523 - acc: 0.2818 - val_loss: 1.8575 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.25352\n",
      "Epoch 18/60\n",
      " - 23s - loss: 1.8353 - acc: 0.2840 - val_loss: 1.8281 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.25352\n",
      "Epoch 19/60\n",
      " - 23s - loss: 1.8364 - acc: 0.2840 - val_loss: 1.8784 - val_acc: 0.2113\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.25352\n",
      "Epoch 20/60\n",
      " - 23s - loss: 1.8409 - acc: 0.2729 - val_loss: 1.8061 - val_acc: 0.2817\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.25352 to 0.28169, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 21/60\n",
      " - 23s - loss: 1.8104 - acc: 0.2810 - val_loss: 1.7899 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.28169\n",
      "Epoch 22/60\n",
      " - 23s - loss: 1.7988 - acc: 0.3063 - val_loss: 1.7796 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.28169\n",
      "Epoch 23/60\n",
      " - 23s - loss: 1.8003 - acc: 0.2892 - val_loss: 1.8078 - val_acc: 0.2958\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.28169 to 0.29577, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 24/60\n",
      " - 24s - loss: 1.7903 - acc: 0.3078 - val_loss: 1.8477 - val_acc: 0.2817\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.29577\n",
      "Epoch 25/60\n",
      " - 23s - loss: 1.7920 - acc: 0.2944 - val_loss: 1.7891 - val_acc: 0.3239\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.29577 to 0.32394, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 26/60\n",
      " - 23s - loss: 1.7635 - acc: 0.3197 - val_loss: 1.7852 - val_acc: 0.3380\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.32394 to 0.33803, saving model to F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\n",
      "Epoch 27/60\n"
     ]
    }
   ],
   "source": [
    "# saved model checkpoint file\n",
    "best_model_file=r\"F:\\speech_project\\Dipali & Sharmila Project\\best_model_trained.hdf5\"\n",
    "\n",
    "MAX_PATIENT=12\n",
    "MAX_EPOCHS=60\n",
    "MAX_BATCH=32\n",
    "\n",
    "# callbacks\n",
    "# removed EarlyStopping(patience=MAX_PATIENT)\n",
    "callback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=2),\n",
    "          ModelCheckpoint(filepath=best_model_file, monitor='val_acc',\n",
    "                          verbose=2, save_best_only=True)]\n",
    "\n",
    "print (\"training started..... please wait.\")\n",
    "# training\n",
    "history=model.fit(X_train, Y_train, \n",
    "                  batch_size=MAX_BATCH, \n",
    "                  epochs=MAX_EPOCHS,\n",
    "                  verbose=2,\n",
    "                  validation_data=(X_test,Y_test),\n",
    "                  callbacks=callback) \n",
    "\n",
    "print (\"training finised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vot1=model.predict_classes(X_test)\n",
    "print(\"Accuracy=\",accuracy_score(y_test, y_pred_vot1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# SAVING THE MODEL\n",
    "import pickle\n",
    "pickle.dump(model, open(\"C:\\\\Users\\\\dbda\\\\Desktop\\\\project\\\\LSTM.pkl\", 'wb'))\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Prediction on unseen data\n",
    "def load_test_data():\n",
    "    x_test,y_test=[],[]\n",
    "    \n",
    "    file=\"F:\\\\speech_project\\\\UnSeenSet\\\\03-01-02-01-02-01-18.wav\"\n",
    "    emotion=emotions[file.split(\"-\")[2]]\n",
    "    feature=extract_feature(file, mfcc=True, chroma=True,spectral_centroid=True,spectral_bandwidth=True,\n",
    "                            spectral_rolloff=True,spectral_contrast=True,rms=True,spectral_flatness=True)\n",
    "    \n",
    "    x_test.append(feature)\n",
    "    y_test.append(emotion)\n",
    "    print(x_test)\n",
    "    return x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data,y_test_data=load_test_data()\n",
    "print(y_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########### LOADING SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "loaded_model = pickle.load(open(\"C:\\\\Users\\\\dbda\\\\Desktop\\\\project\\\\LSTM.pkl\", 'rb'))\n",
    "result = loaded_model.predict(X_test_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le.inverse_transform(result)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
